{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "#import torchvision.datasets as dset\n",
    "#import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from basic import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# For this cell used same code from PyTorch notebook in assignment 2 of Stanford's CS231n Spring 2018 offering\n",
    "preprocessData = False # To preprocess data set this to True\n",
    "USE_GPU = False\n",
    "dtype = torch.float32\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    dtype = torch.float32\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 1\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next two cells, code belongs to [1]. Minor changes made to accomodate to our use \n",
    "# (Using PyTorch instead of Keras/tensorflow)\n",
    "IMG_WIDTH = 28\n",
    "IMG_HEIGHT = 28\n",
    "IMG_CHANNELS = 1\n",
    "PATH = './'\n",
    "epsilon = 1e-12 #For numerical stability\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='skimage')\n",
    "seed = 1\n",
    "random.seed = seed\n",
    "np.random.seed = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (364984, 1, 28, 28)\n",
      "Y_train shape: (364984,)\n",
      "X_dev shape: (121661, 1, 28, 28)\n",
      "Y_dev shape: (121661,)\n"
     ]
    }
   ],
   "source": [
    "trainCSV = \"npy_data/train_npy.csv\"\n",
    "trainDF = pd.read_csv(trainCSV, header = 0)\n",
    "trainDF = trainDF.values\n",
    "X_t = trainDF[:, 0:-5]\n",
    "X_train = np.zeros((len(X_t), 1, 28, 28), dtype= np.float32)\n",
    "for i, row in enumerate(X_t):\n",
    "    X_train[i] = np.reshape(X_t[i, :], (1, 28, 28))\n",
    "# want shape [samples, 1, 28, 28]\n",
    "Y_t = trainDF[:, -5:]   #label x sample\n",
    "Y_train = np.zeros((Y_t.shape[0]))\n",
    "# Pytorch needs indices\n",
    "for i, row in enumerate(Y_t):\n",
    "    Y_train[i] = np.argmax(row)\n",
    "    \n",
    "    \n",
    "devCSV = \"npy_data/dev_npy.csv\"\n",
    "devDF = pd.read_csv(devCSV, header = 0)\n",
    "devDF = devDF.values\n",
    "X_d = devDF[:, 0:-5]\n",
    "X_dev = np.zeros((len(X_d), 1, 28, 28), dtype= np.float32)\n",
    "for i, row in enumerate(X_d):\n",
    "    X_dev[i] = np.reshape(X_d[i, :], (1, 28, 28))\n",
    "Y_d = devDF[:, -5:]   #label x sample\n",
    "Y_dev = np.zeros((Y_d.shape[0]))\n",
    "for i, row in enumerate(Y_d):\n",
    "    Y_train[i] = np.argmax(row)\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_dev shape: \" + str(X_dev.shape))\n",
    "print (\"Y_dev shape: \" + str(Y_dev.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showVisualComparisons(X, y, ex):\n",
    "    plt.imshow(np.uint8(np.reshape(X[ex, :], (28, 28))))\n",
    "    plt.show()\n",
    "    print(Y_train[:, ex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, x_train, y_train, optimizer, epochs = 1, mini_batch_size = 64, noVal = False):\n",
    "#     model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    T = 0\n",
    "    num_batches = int(len(x_train)/mini_batch_size)\n",
    "    num_remaining = len(x_train) - num_batches * mini_batch_size\n",
    "    loss_history = []\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        correct = 0\n",
    "        for t in range(num_batches):\n",
    "            rand_indices = np.random.choice(len(x_train), mini_batch_size)\n",
    "            x = torch.from_numpy(x_train[rand_indices, :, :, :])\n",
    "            y = torch.from_numpy(y_train[rand_indices])\n",
    "            model.train()  # put model to training mode\n",
    "#             x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "#             y = y.to(device=device, dtype=dtype)\n",
    "            y = y.type(torch.LongTensor)\n",
    "\n",
    "            preds = model(x)\n",
    "            _, predicted = torch.max(preds.data, 1)\n",
    "            \n",
    "            for i in range(len(predicted)):\n",
    "                if predicted[i] == y[i]:\n",
    "                    correct += 1\n",
    "            \n",
    "            loss = F.cross_entropy(preds, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if T % print_every == 0:\n",
    "                currLoss = loss.item()\n",
    "                loss_history.append(currLoss)\n",
    "#                 print('Epoch %d, Iteration %d, loss = %.4f' % (e, t, currLoss))\n",
    "            if (num_remaining <= 0 and t == (num_batches -1)):\n",
    "#                 perf = calculatePerformance(x_train, y_train, model)\n",
    "                perf = (correct / (float(mini_batch_size)))\n",
    "                print('Train performance at epoch %d is %.4f' % (e, perf))\n",
    "                if (noVal == False):\n",
    "#                     perf = calculatePerformance(X_val, Y_val, model)\n",
    "                    print('Val performance at epoch %d is %.4f' % (e, perf))\n",
    "            T +=1\n",
    "        if num_remaining > 0:\n",
    "            rand_indices = np.random.choice(len(x_train), num_remaining)\n",
    "\n",
    "            x = torch.from_numpy(x_train[rand_indices, :, :, :])\n",
    "            y = torch.from_numpy(y_train[rand_indices])\n",
    "            model.train()  # put model to training mode\n",
    "#             x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "#             y = y.to(device=device, dtype=dtype)\n",
    "            y = y.type(torch.LongTensor)\n",
    "            \n",
    "\n",
    "\n",
    "            preds = model(x)\n",
    "            _, predicted = torch.max(preds.data, 1)\n",
    "            #values, indices = torch.max(preds, 1)\n",
    "            \n",
    "            for i in range(len(predicted)):\n",
    "                if predicted[i] == y[i]:\n",
    "                    correct += 1\n",
    "                    \n",
    "            loss = F.cross_entropy(preds, y)\n",
    "            #loss(preds, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "            if T % print_every == 0:\n",
    "                currLoss = loss.item()\n",
    "                loss_history.append(currLoss)\n",
    "#                 print('Epoch %d, Iteration %d, loss = %.4f' % (e, num_batches, currLoss))\n",
    "#             perf = (correct/(float(len(x_train))))\n",
    "#             print('Train performance at epoch %d is %.4f' % (e, perf))\n",
    "            if (noVal == False):\n",
    "#                 perf = (correct/(float(len(x_train))))\n",
    "\n",
    "                print('Val performance at epoch %d is %.4f' % (e, perf))\n",
    "            T +=1\n",
    "        perf = (correct / len(x_train))\n",
    "        print('Train performance at epoch %d is %.4f' % (e, perf))\n",
    "    return perf, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying out learning rate of  4.6653163946557273e-05\n",
      "Train performance at epoch 0 is 0.5980\n",
      "Train performance at epoch 1 is 0.6619\n",
      "Train performance at epoch 2 is 0.6703\n",
      "Train performance at epoch 3 is 0.6751\n",
      "Train performance at epoch 4 is 0.6785\n",
      "Train performance at epoch 5 is 0.6801\n",
      "Train performance at epoch 6 is 0.6825\n",
      "Train performance at epoch 7 is 0.6845\n",
      "Train performance at epoch 8 is 0.6848\n",
      "Train performance at epoch 9 is 0.6869\n",
      "Train performance at epoch 10 is 0.6878\n",
      "Train performance at epoch 11 is 0.6884\n",
      "Train performance at epoch 12 is 0.6892\n",
      "Train performance at epoch 13 is 0.6894\n",
      "Train performance at epoch 14 is 0.6928\n",
      "Train performance at epoch 15 is 0.6902\n",
      "Train performance at epoch 16 is 0.6908\n",
      "Train performance at epoch 17 is 0.6943\n",
      "Train performance at epoch 18 is 0.6941\n",
      "Train performance at epoch 19 is 0.6933\n",
      "Train performance at epoch 20 is 0.6949\n",
      "Train performance at epoch 21 is 0.6952\n",
      "Train performance at epoch 22 is 0.6944\n",
      "Train performance at epoch 23 is 0.6978\n",
      "Train performance at epoch 24 is 0.6966\n",
      "Train performance at epoch 25 is 0.6993\n",
      "Train performance at epoch 26 is 0.6970\n",
      "Train performance at epoch 27 is 0.6994\n",
      "Train performance at epoch 28 is 0.6996\n",
      "Train performance at epoch 29 is 0.6999\n",
      "Train performance at epoch 30 is 0.7018\n",
      "Train performance at epoch 31 is 0.7006\n",
      "Train performance at epoch 32 is 0.7013\n",
      "Train performance at epoch 33 is 0.7011\n",
      "Train performance at epoch 34 is 0.7016\n",
      "Train performance at epoch 35 is 0.7026\n",
      "Train performance at epoch 36 is 0.7033\n",
      "Train performance at epoch 37 is 0.7027\n",
      "Train performance at epoch 38 is 0.7052\n",
      "Train performance at epoch 39 is 0.7049\n",
      "Train performance at epoch 40 is 0.7057\n",
      "Train performance at epoch 41 is 0.7051\n",
      "Train performance at epoch 42 is 0.7060\n",
      "Train performance at epoch 43 is 0.7057\n",
      "Train performance at epoch 44 is 0.7051\n",
      "Train performance at epoch 45 is 0.7077\n",
      "Train performance at epoch 46 is 0.7076\n",
      "Train performance at epoch 47 is 0.7061\n",
      "Train performance at epoch 48 is 0.7065\n",
      "Train performance at epoch 49 is 0.7084\n",
      "Trying out learning rate of  0.002770522821905681\n",
      "Train performance at epoch 0 is 0.6347\n",
      "Train performance at epoch 1 is 0.6468\n",
      "Train performance at epoch 2 is 0.6482\n",
      "Train performance at epoch 3 is 0.6485\n",
      "Train performance at epoch 4 is 0.6482\n",
      "Train performance at epoch 5 is 0.6469\n",
      "Train performance at epoch 6 is 0.6466\n",
      "Train performance at epoch 7 is 0.6460\n",
      "Train performance at epoch 8 is 0.6457\n",
      "Train performance at epoch 9 is 0.6468\n",
      "Train performance at epoch 10 is 0.6477\n",
      "Train performance at epoch 11 is 0.6450\n",
      "Train performance at epoch 12 is 0.6440\n",
      "Train performance at epoch 13 is 0.6446\n",
      "Train performance at epoch 14 is 0.6469\n",
      "Train performance at epoch 15 is 0.6462\n",
      "Train performance at epoch 16 is 0.6441\n",
      "Train performance at epoch 17 is 0.6453\n",
      "Train performance at epoch 18 is 0.6464\n",
      "Train performance at epoch 19 is 0.6454\n",
      "Train performance at epoch 20 is 0.6458\n",
      "Train performance at epoch 21 is 0.6435\n",
      "Train performance at epoch 22 is 0.6438\n",
      "Train performance at epoch 23 is 0.6477\n",
      "Train performance at epoch 24 is 0.6469\n",
      "Train performance at epoch 25 is 0.6489\n",
      "Train performance at epoch 26 is 0.6467\n",
      "Train performance at epoch 27 is 0.6443\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-8cfc59e7e4dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mmodelPerf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoVal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mlossHistories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelPerf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodelPerf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelPerf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbestLoss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-492ec270f4bd>\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(model, x_train, y_train, optimizer, epochs, mini_batch_size, noVal)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# Actually update the parameters of the model using the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# computed by the backwards pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs229/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Overfitting data first\n",
    "bestPerf = -1\n",
    "lossHistory = None\n",
    "lossHistories = {}\n",
    "print_every = 1\n",
    "bestModel = None\n",
    "bestLoss = 10000\n",
    "lrUsed = 0\n",
    "x_train = X_train[0:500, :, :, :]\n",
    "y_train = Y_train[0:500]\n",
    "lrs = []\n",
    "lrs.append(4.6653163946557273e-05)\n",
    "for i in range(3):\n",
    "    lrs.append(5*np.random.rand()*1e-3)\n",
    "# lrs = [1e-7,1e-6,1e-5,1e-4,1e-3]\n",
    "lrs.append(.002147418314081924) # Best results from last random searches \n",
    "for lr in lrs:\n",
    "    print('Trying out learning rate of ', lr)\n",
    "    model = NNet()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "    modelPerf = trainModel(model, X_train, Y_train, optimizer, epochs = 50, noVal = True)\n",
    "    lossHistories[str(lr)] = modelPerf[1]\n",
    "    if modelPerf[1][len(modelPerf[1])-1] < bestLoss:\n",
    "        bestLoss = modelPerf[1][len(modelPerf[1])-1]\n",
    "        bestPerf = modelPerf[0]\n",
    "        lossHistory = modelPerf[1]\n",
    "        bestModel = model\n",
    "        lrUsed = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
