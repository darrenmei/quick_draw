{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "#import torchvision.datasets as dset\n",
    "#import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from basic import *\n",
    "from cnn7 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# For this cell used same code from PyTorch notebook in assignment 2 of Stanford's CS231n Spring 2018 offering\n",
    "preprocessData = False # To preprocess data set this to True\n",
    "USE_GPU = False\n",
    "dtype = torch.float32\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    dtype = torch.float32\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 1\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e6ea41b05832>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Next two cells, code belongs to [1]. Minor changes made to accomodate to our use \n",
    "# (Using PyTorch instead of Keras/tensorflow)\n",
    "IMG_WIDTH = 28\n",
    "IMG_HEIGHT = 28\n",
    "IMG_CHANNELS = 1\n",
    "PATH = './'\n",
    "epsilon = 1e-12 #For numerical stability\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='skimage')\n",
    "seed = 1\n",
    "random.seed = seed\n",
    "np.random.seed = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (364984, 1, 28, 28)\n",
      "Y_train shape: (364984,)\n",
      "X_dev shape: (121661, 1, 28, 28)\n",
      "Y_dev shape: (121661,)\n"
     ]
    }
   ],
   "source": [
    "trainCSV = \"npy_data/train_npy.csv\"\n",
    "trainDF = pd.read_csv(trainCSV, header = 0)\n",
    "trainDF = trainDF.values\n",
    "X_t = trainDF[:, 0:-5]\n",
    "X_train = np.zeros((len(X_t), 1, 28, 28), dtype= np.float32)\n",
    "for i, row in enumerate(X_t):\n",
    "    X_train[i] = np.reshape(X_t[i, :], (1, 28, 28))\n",
    "# want shape [samples, 1, 28, 28]\n",
    "Y_t = trainDF[:, -5:]   #label x sample\n",
    "Y_train = np.zeros((Y_t.shape[0]))\n",
    "# Pytorch needs indices\n",
    "for i, row in enumerate(Y_t):\n",
    "    Y_train[i] = np.argmax(row)\n",
    "    \n",
    "    \n",
    "devCSV = \"npy_data/dev_npy.csv\"\n",
    "devDF = pd.read_csv(devCSV, header = 0)\n",
    "devDF = devDF.values\n",
    "X_d = devDF[:, 0:-5]\n",
    "X_dev = np.zeros((len(X_d), 1, 28, 28), dtype= np.float32)\n",
    "for i, row in enumerate(X_d):\n",
    "    X_dev[i] = np.reshape(X_d[i, :], (1, 28, 28))\n",
    "Y_d = devDF[:, -5:]   #label x sample\n",
    "Y_dev = np.zeros((Y_d.shape[0]))\n",
    "for i, row in enumerate(Y_d):\n",
    "    Y_dev[i] = np.argmax(row)\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_dev shape: \" + str(X_dev.shape))\n",
    "print (\"Y_dev shape: \" + str(Y_dev.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showVisualComparisons(X, y, ex):\n",
    "    plt.imshow(np.uint8(np.reshape(X[ex, :], (28, 28))))\n",
    "    plt.show()\n",
    "    print(Y_train[:, ex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, x_train, y_train, optimizer, epochs = 1, mini_batch_size = 64, noVal = False):\n",
    "#     model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    T = 0\n",
    "    num_batches = int(len(x_train)/mini_batch_size)\n",
    "    num_remaining = len(x_train) - num_batches * mini_batch_size\n",
    "    loss_history = []\n",
    "    epsilon = 0.0\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        correct = 0\n",
    "        for t in range(num_batches):\n",
    "            rand_indices = np.random.choice(len(x_train), mini_batch_size)\n",
    "            x = torch.from_numpy(x_train[rand_indices, :, :, :])\n",
    "            y = torch.from_numpy(y_train[rand_indices])\n",
    "            model.train()  # put model to training mode\n",
    "#             x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "#             y = y.to(device=device, dtype=dtype)\n",
    "            y = y.type(torch.LongTensor)\n",
    "\n",
    "            preds = model(x)\n",
    "            preds = torch.squeeze(preds)\n",
    "#             print(preds.size())\n",
    "            _, predicted = torch.max(preds.data, 1)\n",
    "            \n",
    "            for i in range(len(predicted)):\n",
    "                if predicted[i] == y[i]:\n",
    "                    correct += 1\n",
    "            \n",
    "            loss = F.cross_entropy(preds, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if T % print_every == 0:\n",
    "                currLoss = loss.item()\n",
    "                loss_history.append(currLoss)\n",
    "#                 print('Epoch %d, Iteration %d, loss = %.4f' % (e, t, currLoss))\n",
    "            if (num_remaining <= 0 and t == (num_batches -1)):\n",
    "#                 perf = calculatePerformance(x_train, y_train, model)\n",
    "                perf = (correct / (float(mini_batch_size)))\n",
    "                print('Train performance at epoch %d is %.4f' % (e, perf))\n",
    "                if (noVal == False):\n",
    "#                     perf = calculatePerformance(X_val, Y_val, model)\n",
    "                    print('Val performance at epoch %d is %.4f' % (e, perf))\n",
    "            T +=1\n",
    "        if num_remaining > 0:\n",
    "            rand_indices = np.random.choice(len(x_train), num_remaining)\n",
    "\n",
    "            x = torch.from_numpy(x_train[rand_indices, :, :, :])\n",
    "            y = torch.from_numpy(y_train[rand_indices])\n",
    "            model.train()  # put model to training mode\n",
    "#             x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "#             y = y.to(device=device, dtype=dtype)\n",
    "            y = y.type(torch.LongTensor)\n",
    "            \n",
    "\n",
    "\n",
    "            preds = model(x)\n",
    "            preds = torch.squeeze(preds)\n",
    "            print(preds.size())\n",
    "            \n",
    "            _, predicted = torch.max(preds.data, 1)\n",
    "            #values, indices = torch.max(preds, 1)\n",
    "            \n",
    "            for i in range(len(predicted)):\n",
    "                if predicted[i] == y[i]:\n",
    "                    correct += 1\n",
    "                    \n",
    "            loss = F.cross_entropy(preds, y)\n",
    "            #loss(preds, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "            if T % print_every == 0:\n",
    "                currLoss = loss.item()\n",
    "                loss_history.append(currLoss)\n",
    "#                 print('Epoch %d, Iteration %d, loss = %.4f' % (e, num_batches, currLoss))\n",
    "#             perf = (correct/(float(len(x_train))))\n",
    "#             print('Train performance at epoch %d is %.4f' % (e, perf))\n",
    "            if (noVal == False):\n",
    "#                 perf = (correct/(float(len(x_train))))\n",
    "\n",
    "                print('Val performance at epoch %d is %.4f' % (e, perf))\n",
    "            T +=1\n",
    "        perf = (correct / len(x_train))\n",
    "        print(loss_history[-1])\n",
    "        print('Train performance at epoch %d is %.4f' % (e, perf))\n",
    "        if loss_history[-1] <= epsilon:\n",
    "            break\n",
    "    return perf, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying out learning rate of  8.421693197665337e-05\n",
      "(50, 1, 28, 28)\n",
      "(50,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-76180162edf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmodelPerf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoVal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mlossHistories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelPerf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodelPerf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelPerf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbestLoss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-46a336c1d806>\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(model, x_train, y_train, optimizer, epochs, mini_batch_size, noVal)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# This is the backwards pass: compute the gradient of the loss with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# respect to each  parameter of the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Actually update the parameters of the model using the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs229/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs229/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Overfitting data first\n",
    "bestPerf = -1\n",
    "lossHistory = None\n",
    "lossHistories = {}\n",
    "print_every = 1\n",
    "bestModel = None\n",
    "bestLoss = 10000\n",
    "lrUsed = 0\n",
    "x_train = X_train[0:50, :, :, :]\n",
    "y_train = Y_train[0:50]\n",
    "lrs = []\n",
    "# lrs.append(4.6653163946557273e-06)\n",
    "# lrs.append(0.0005)\n",
    "# lrs.append(0.00005)\n",
    "lrs.append(8.421693197665337e-05)\n",
    "for i in range(3):\n",
    "    lrs.append(5*np.random.rand()*1e-4)\n",
    "# lrs = [1e-7,1e-6,1e-5,1e-4,1e-3]\n",
    "lrs.append(.002147418314081924) # Best results from last random searches \n",
    "for lr in lrs:\n",
    "    print('Trying out learning rate of ', lr)\n",
    "    model = CNNNet()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "    print(np.shape(x_train))\n",
    "    print(np.shape(y_train))\n",
    "    modelPerf = trainModel(model, X_train, Y_train, optimizer, epochs = 50, noVal = True)\n",
    "    lossHistories[str(lr)] = modelPerf[1]\n",
    "    if modelPerf[1][len(modelPerf[1])-1] < bestLoss:\n",
    "        bestLoss = modelPerf[1][len(modelPerf[1])-1]\n",
    "        bestPerf = modelPerf[0]\n",
    "        lossHistory = modelPerf[1]\n",
    "        bestModel = model\n",
    "        lrUsed = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
