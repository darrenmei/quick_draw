{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from ast import literal_eval\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "#import torchvision.datasets as dset\n",
    "#import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from basic import *\n",
    "from rnn_lstm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# For this cell used same code from PyTorch notebook in assignment 2 of Stanford's CS231n Spring 2018 offering\n",
    "preprocessData = False # To preprocess data set this to True\n",
    "USE_GPU = False\n",
    "dtype = torch.float32\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    dtype = torch.float32\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 1\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next two cells, code belongs to [1]. Minor changes made to accomodate to our use \n",
    "# (Using PyTorch instead of Keras/tensorflow)\n",
    "STROKE_COUNT = 196\n",
    "IMG_WIDTH = 28\n",
    "IMG_HEIGHT = 28\n",
    "IMG_CHANNELS = 1\n",
    "PATH = './'\n",
    "epsilon = 1e-12 #For numerical stability\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='skimage')\n",
    "seed = 1\n",
    "random.seed = seed\n",
    "np.random.seed = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_it(raw_strokes):\n",
    "    \"\"\"preprocess the string and make\n",
    "    a standard Nx3 stroke vector\"\"\"\n",
    "\n",
    "    stroke_vec = literal_eval(raw_strokes)# string->list\n",
    "    #print('stroke_vec: ', stroke_vec)\n",
    "    # unwrap the list\n",
    "    in_strokes = [(xi,yi,i)\n",
    "    for i,(x,y) in enumerate(stroke_vec)\n",
    "    for xi,yi in zip(x,y)]\n",
    "    c_strokes = np.stack(in_strokes)\n",
    "    # replace stroke id with 1 for continue, 2 for new\n",
    "    c_strokes[:,2] = [1]+np.diff(c_strokes[:,2]).tolist()\n",
    "    c_strokes[:,2] += 1 # since 0 is no stroke\n",
    "    # pad the strokes with zeros\n",
    "    return pad_sequences(c_strokes.swapaxes(0, 1),\n",
    "                         maxlen=STROKE_COUNT,\n",
    "                         padding='post').swapaxes(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateLabels(data):\n",
    "    # Key: col 1: airplane, col 2: campfire, col 3: key, col 4: moon, col 5: palm tree\n",
    "    #print('data: ', data)\n",
    "    m = data.shape[0]\n",
    "    print('m: ', m)\n",
    "    labels = np.zeros((m, 5))\n",
    "    for row in range(m):\n",
    "        if row == 0:\n",
    "            pass\n",
    "        if data[row] == 'airplane':\n",
    "            labels[row, 0] = 1\n",
    "        elif data[row] == 'campfire':\n",
    "            labels[row, 1] = 1\n",
    "        elif data[row] == 'key':\n",
    "            labels[row, 2] = 1\n",
    "        elif data[row] == 'moon':\n",
    "            labels[row, 3] = 1\n",
    "        else:\n",
    "            labels[row, 4] = 1\n",
    "    print('labels: ', labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Strokes(data):\n",
    "    \"\"\"\n",
    "    Function looks at the dataframe and returns matrix of description\n",
    "    condensed encodings for all samples (each sample has variable length vectors\n",
    "        of indices corresponding to vocab indices of words that appear in each\n",
    "        description)\n",
    "    Arguments:\n",
    "    df -- dataframe of data\n",
    "    Returns:\n",
    "    X -- product descriptions\n",
    "    \"\"\"    \n",
    "    m = data.shape[0]\n",
    "    #print('data: ', data)\n",
    "\n",
    "    # Key: col 1: airplane, col 2: campfire, col 3: key, col 4: moon, col 5: palm tree\n",
    "    #print('data: ', data)\n",
    "\n",
    "    temp = []\n",
    "    for i in range(data.shape[0]):\n",
    "        temp.append(stack_it(data[i, 0]))\n",
    "    X = np.asarray(temp)\n",
    "\n",
    "    #Y= np.array(Y)\n",
    "    X = np.asarray(X)\n",
    "    print('X: ', X)\n",
    "    print('X shape: ', X.shape)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (364984, 1, 28, 28)\n",
      "Y_train shape: (364984,)\n",
      "X_dev shape: (121661, 1, 28, 28)\n",
      "Y_dev shape: (121661,)\n"
     ]
    }
   ],
   "source": [
    "# Load train data\n",
    "trainCSV = \"./train.csv\"\n",
    "trainDF = pd.read_csv(trainCSV, header = 0)\n",
    "trainDF = trainDF.values\n",
    "X_strokes = Strokes(trainDF)\n",
    "X_train = np.swapaxes(X_strokes, 1, 2)\n",
    "\n",
    "Y_t = trainDF[:, 2]\n",
    "Y_labels = CreateLabels(Y_t)\n",
    "Y_train = np.zeros((Y_labels.shape[0]))\n",
    "# Pytorch needs indices\n",
    "for i, row in enumerate(Y_labels):\n",
    "    Y_train[i] = np.argmax(row)\n",
    "\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dev data\n",
    "devCSV = \"./dev.csv\"\n",
    "devDF = pd.read_csv(devCSV, header = 0)\n",
    "devDF = devDF.values\n",
    "X_strokes = Strokes(devDF)\n",
    "X_dev = np.swapaxes(X_strokes, 1, 2)\n",
    "\n",
    "Y_d = devDF[:, 2]\n",
    "Y_labels = CreateLabels(Y_d)\n",
    "Y_dev = np.zeros((Y_labels.shape[0]))\n",
    "# Pytorch needs indices\n",
    "for i, row in enumerate(Y_labels):\n",
    "    Y_dev[i] = np.argmax(row)\n",
    "\n",
    "print (\"X_dev shape: \" + str(X_dev.shape))\n",
    "print (\"Y_dev shape: \" + str(Y_dev.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showVisualComparisons(X, y, ex):\n",
    "    plt.imshow(np.uint8(np.reshape(X[ex, :], (28, 28))))\n",
    "    plt.show()\n",
    "    print(Y_train[:, ex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, x_train, y_train, optimizer, epochs = 1, mini_batch_size = 64, noVal = False):\n",
    "#     model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    T = 0\n",
    "    num_batches = int(len(x_train)/mini_batch_size)\n",
    "    num_remaining = len(x_train) - num_batches * mini_batch_size\n",
    "    loss_history = []\n",
    "    epsilon = 0.0\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        correct = 0\n",
    "        for t in range(num_batches):\n",
    "            rand_indices = np.random.choice(len(x_train), mini_batch_size)\n",
    "            x = torch.from_numpy(x_train[rand_indices, :, :, :])\n",
    "            y = torch.from_numpy(y_train[rand_indices])\n",
    "            model.train()  # put model to training mode\n",
    "#             x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "#             y = y.to(device=device, dtype=dtype)\n",
    "            y = y.type(torch.LongTensor)\n",
    "\n",
    "            preds = model(x)\n",
    "            preds = torch.squeeze(preds)\n",
    "#             print(preds.size())\n",
    "            _, predicted = torch.max(preds.data, 1)\n",
    "            \n",
    "            for i in range(len(predicted)):\n",
    "                if predicted[i] == y[i]:\n",
    "                    correct += 1\n",
    "            \n",
    "            loss = F.cross_entropy(preds, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if T % print_every == 0:\n",
    "                currLoss = loss.item()\n",
    "                loss_history.append(currLoss)\n",
    "#                 print('Epoch %d, Iteration %d, loss = %.4f' % (e, t, currLoss))\n",
    "            if (num_remaining <= 0 and t == (num_batches -1)):\n",
    "#                 perf = calculatePerformance(x_train, y_train, model)\n",
    "                perf = (correct / (float(mini_batch_size)))\n",
    "                print('Train performance at epoch %d is %.4f' % (e, perf))\n",
    "                if (noVal == False):\n",
    "#                     perf = calculatePerformance(X_val, Y_val, model)\n",
    "                    print('Val performance at epoch %d is %.4f' % (e, perf))\n",
    "            T +=1\n",
    "        if num_remaining > 0:\n",
    "            rand_indices = np.random.choice(len(x_train), num_remaining)\n",
    "\n",
    "            x = torch.from_numpy(x_train[rand_indices, :, :, :])\n",
    "            y = torch.from_numpy(y_train[rand_indices])\n",
    "            model.train()  # put model to training mode\n",
    "#             x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "#             y = y.to(device=device, dtype=dtype)\n",
    "            y = y.type(torch.LongTensor)\n",
    "            \n",
    "\n",
    "\n",
    "            preds = model(x)\n",
    "            preds = torch.squeeze(preds)\n",
    "            print(preds.size())\n",
    "            \n",
    "            _, predicted = torch.max(preds.data, 1)\n",
    "            #values, indices = torch.max(preds, 1)\n",
    "            \n",
    "            for i in range(len(predicted)):\n",
    "                if predicted[i] == y[i]:\n",
    "                    correct += 1\n",
    "                    \n",
    "            loss = F.cross_entropy(preds, y)\n",
    "            #loss(preds, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "            if T % print_every == 0:\n",
    "                currLoss = loss.item()\n",
    "                loss_history.append(currLoss)\n",
    "#                 print('Epoch %d, Iteration %d, loss = %.4f' % (e, num_batches, currLoss))\n",
    "#             perf = (correct/(float(len(x_train))))\n",
    "#             print('Train performance at epoch %d is %.4f' % (e, perf))\n",
    "            if (noVal == False):\n",
    "#                 perf = (correct/(float(len(x_train))))\n",
    "\n",
    "                print('Val performance at epoch %d is %.4f' % (e, perf))\n",
    "            T +=1\n",
    "        perf = (correct / len(x_train))\n",
    "        print(loss_history[-1])\n",
    "        print('Train performance at epoch %d is %.4f' % (e, perf))\n",
    "        if loss_history[-1] <= epsilon:\n",
    "            break\n",
    "    return perf, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying out learning rate of  8.421693197665337e-05\n",
      "(1000, 1, 28, 28)\n",
      "(1000,)\n",
      "torch.Size([40, 5])\n",
      "509.18194580078125\n",
      "Train performance at epoch 0 is 0.2400\n",
      "torch.Size([40, 5])\n",
      "24.599872589111328\n",
      "Train performance at epoch 1 is 0.5430\n",
      "torch.Size([40, 5])\n",
      "10.25886344909668\n",
      "Train performance at epoch 2 is 0.7680\n",
      "torch.Size([40, 5])\n",
      "5.457827091217041\n",
      "Train performance at epoch 3 is 0.8810\n",
      "torch.Size([40, 5])\n",
      "0.6798503398895264\n",
      "Train performance at epoch 4 is 0.9250\n",
      "torch.Size([40, 5])\n",
      "1.88315749168396\n",
      "Train performance at epoch 5 is 0.9390\n",
      "torch.Size([40, 5])\n",
      "5.0895538330078125\n",
      "Train performance at epoch 6 is 0.9430\n",
      "torch.Size([40, 5])\n",
      "0.47974300384521484\n",
      "Train performance at epoch 7 is 0.9490\n",
      "torch.Size([40, 5])\n",
      "0.07137546688318253\n",
      "Train performance at epoch 8 is 0.9670\n",
      "torch.Size([40, 5])\n",
      "0.04734325408935547\n",
      "Train performance at epoch 9 is 0.9850\n",
      "torch.Size([40, 5])\n",
      "0.2498922348022461\n",
      "Train performance at epoch 10 is 0.9830\n",
      "torch.Size([40, 5])\n",
      "0.0\n",
      "Train performance at epoch 11 is 0.9860\n"
     ]
    }
   ],
   "source": [
    "# Overfitting data first\n",
    "bestPerf = -1\n",
    "lossHistory = None\n",
    "lossHistories = {}\n",
    "print_every = 1\n",
    "bestModel = None\n",
    "bestLoss = 10000\n",
    "lrUsed = 0\n",
    "half_X_train = X_train[0:len(X_train) // 2, :, :, :]\n",
    "half_Y_train = Y_train[0:len(Y_train) // 2]\n",
    "x_train = X_train[0:1000, :, :, :]\n",
    "y_train = Y_train[0:1000]\n",
    "lrs = []\n",
    "# lrs.append(.002147418314081924) # best basic \n",
    "# lrs.append(0.0003513721088531244) #found another good basic\n",
    "lrs.append(8.421693197665337e-05) # best conv\n",
    "# for i in range(3):\n",
    "#     lrs.append(5*np.random.rand()*1e-4)\n",
    "# lrs = [1e-7,1e-6,1e-5,1e-4,1e-3]\n",
    "for lr in lrs:\n",
    "    print('Trying out learning rate of ', lr)\n",
    "    model = CNNNet()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "    print(np.shape(x_train))\n",
    "    print(np.shape(y_train))\n",
    "    modelPerf = trainModel(model, x_train, y_train, optimizer, epochs = 50, noVal = True)\n",
    "    lossHistories[str(lr)] = modelPerf[1]\n",
    "    if modelPerf[1][len(modelPerf[1])-1] < bestLoss:\n",
    "        bestLoss = modelPerf[1][len(modelPerf[1])-1]\n",
    "        bestPerf = modelPerf[0]\n",
    "        lossHistory = modelPerf[1]\n",
    "        bestModel = model\n",
    "        lrUsed = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch:  0\n",
      "starting batch:  1\n",
      "starting batch:  2\n",
      "starting batch:  3\n",
      "starting batch:  4\n",
      "starting batch:  5\n",
      "starting batch:  6\n",
      "starting batch:  7\n",
      "starting batch:  8\n",
      "starting batch:  9\n",
      "starting batch:  10\n",
      "starting batch:  11\n",
      "starting batch:  12\n",
      "starting batch:  13\n",
      "starting batch:  14\n",
      "starting batch:  15\n",
      "starting batch:  16\n",
      "starting batch:  17\n",
      "starting batch:  18\n",
      "starting batch:  19\n",
      "starting batch:  20\n",
      "starting batch:  21\n",
      "starting batch:  22\n",
      "starting batch:  23\n",
      "starting batch:  24\n",
      "starting batch:  25\n",
      "starting batch:  26\n",
      "starting batch:  27\n",
      "starting batch:  28\n",
      "starting batch:  29\n",
      "starting batch:  30\n",
      "starting batch:  31\n",
      "starting batch:  32\n",
      "starting batch:  33\n",
      "starting batch:  34\n",
      "starting batch:  35\n",
      "starting batch:  36\n",
      "starting batch:  37\n",
      "starting batch:  38\n",
      "starting batch:  39\n",
      "starting batch:  40\n",
      "starting batch:  41\n",
      "starting batch:  42\n",
      "starting batch:  43\n",
      "starting batch:  44\n",
      "starting batch:  45\n",
      "starting batch:  46\n",
      "starting batch:  47\n",
      "starting batch:  48\n",
      "starting batch:  49\n",
      "starting batch:  50\n",
      "starting batch:  51\n",
      "starting batch:  52\n",
      "starting batch:  53\n",
      "starting batch:  54\n",
      "starting batch:  55\n",
      "starting batch:  56\n",
      "starting batch:  57\n",
      "starting batch:  58\n",
      "starting batch:  59\n",
      "starting batch:  60\n",
      "starting batch:  61\n",
      "starting batch:  62\n",
      "starting batch:  63\n",
      "starting batch:  64\n",
      "starting batch:  65\n",
      "starting batch:  66\n",
      "starting batch:  67\n",
      "starting batch:  68\n",
      "starting batch:  69\n",
      "starting batch:  70\n",
      "starting batch:  71\n",
      "starting batch:  72\n",
      "starting batch:  73\n",
      "starting batch:  74\n",
      "starting batch:  75\n",
      "starting batch:  76\n",
      "starting batch:  77\n",
      "starting batch:  78\n",
      "starting batch:  79\n",
      "starting batch:  80\n",
      "starting batch:  81\n",
      "starting batch:  82\n",
      "starting batch:  83\n",
      "starting batch:  84\n",
      "starting batch:  85\n",
      "starting batch:  86\n",
      "starting batch:  87\n",
      "starting batch:  88\n",
      "starting batch:  89\n",
      "starting batch:  90\n",
      "starting batch:  91\n",
      "starting batch:  92\n",
      "starting batch:  93\n",
      "starting batch:  94\n",
      "starting batch:  95\n",
      "starting batch:  96\n",
      "starting batch:  97\n",
      "starting batch:  98\n",
      "starting batch:  99\n",
      "starting batch:  100\n",
      "starting batch:  101\n",
      "starting batch:  102\n",
      "starting batch:  103\n",
      "starting batch:  104\n",
      "starting batch:  105\n",
      "starting batch:  106\n",
      "starting batch:  107\n",
      "starting batch:  108\n",
      "starting batch:  109\n",
      "starting batch:  110\n",
      "starting batch:  111\n",
      "starting batch:  112\n",
      "starting batch:  113\n",
      "starting batch:  114\n",
      "starting batch:  115\n",
      "starting batch:  116\n",
      "starting batch:  117\n",
      "starting batch:  118\n",
      "starting batch:  119\n",
      "starting batch:  120\n",
      "starting batch:  121\n",
      "starting batch:  122\n",
      "starting batch:  123\n",
      "starting batch:  124\n",
      "starting batch:  125\n",
      "starting batch:  126\n",
      "starting batch:  127\n",
      "starting batch:  128\n",
      "starting batch:  129\n",
      "starting batch:  130\n",
      "starting batch:  131\n",
      "starting batch:  132\n",
      "starting batch:  133\n",
      "starting batch:  134\n",
      "starting batch:  135\n",
      "starting batch:  136\n",
      "starting batch:  137\n",
      "starting batch:  138\n",
      "starting batch:  139\n",
      "starting batch:  140\n",
      "starting batch:  141\n",
      "starting batch:  142\n",
      "starting batch:  143\n",
      "starting batch:  144\n",
      "starting batch:  145\n",
      "starting batch:  146\n",
      "starting batch:  147\n",
      "starting batch:  148\n",
      "starting batch:  149\n",
      "starting batch:  150\n",
      "starting batch:  151\n",
      "starting batch:  152\n",
      "starting batch:  153\n",
      "starting batch:  154\n",
      "starting batch:  155\n",
      "starting batch:  156\n",
      "starting batch:  157\n",
      "starting batch:  158\n",
      "starting batch:  159\n",
      "starting batch:  160\n",
      "starting batch:  161\n",
      "starting batch:  162\n",
      "starting batch:  163\n",
      "starting batch:  164\n",
      "starting batch:  165\n",
      "starting batch:  166\n",
      "starting batch:  167\n",
      "starting batch:  168\n",
      "starting batch:  169\n",
      "starting batch:  170\n",
      "starting batch:  171\n",
      "starting batch:  172\n",
      "starting batch:  173\n",
      "starting batch:  174\n",
      "starting batch:  175\n",
      "starting batch:  176\n",
      "starting batch:  177\n",
      "starting batch:  178\n",
      "starting batch:  179\n",
      "starting batch:  180\n",
      "starting batch:  181\n",
      "starting batch:  182\n",
      "starting batch:  183\n",
      "starting batch:  184\n",
      "starting batch:  185\n",
      "starting batch:  186\n",
      "starting batch:  187\n",
      "starting batch:  188\n",
      "starting batch:  189\n",
      "starting batch:  190\n",
      "starting batch:  191\n",
      "starting batch:  192\n",
      "starting batch:  193\n",
      "starting batch:  194\n",
      "starting batch:  195\n",
      "starting batch:  196\n",
      "starting batch:  197\n",
      "starting batch:  198\n",
      "starting batch:  199\n",
      "starting batch:  200\n",
      "starting batch:  201\n",
      "starting batch:  202\n",
      "starting batch:  203\n",
      "starting batch:  204\n",
      "starting batch:  205\n",
      "starting batch:  206\n",
      "starting batch:  207\n",
      "starting batch:  208\n",
      "starting batch:  209\n",
      "starting batch:  210\n",
      "starting batch:  211\n",
      "starting batch:  212\n",
      "starting batch:  213\n",
      "starting batch:  214\n",
      "starting batch:  215\n",
      "starting batch:  216\n",
      "starting batch:  217\n",
      "starting batch:  218\n",
      "starting batch:  219\n",
      "starting batch:  220\n",
      "starting batch:  221\n",
      "starting batch:  222\n",
      "starting batch:  223\n",
      "starting batch:  224\n",
      "starting batch:  225\n",
      "starting batch:  226\n",
      "starting batch:  227\n",
      "starting batch:  228\n",
      "starting batch:  229\n",
      "starting batch:  230\n",
      "starting batch:  231\n",
      "starting batch:  232\n",
      "starting batch:  233\n",
      "starting batch:  234\n",
      "starting batch:  235\n",
      "starting batch:  236\n",
      "starting batch:  237\n",
      "starting batch:  238\n",
      "starting batch:  239\n",
      "starting batch:  240\n",
      "starting batch:  241\n",
      "starting batch:  242\n",
      "starting batch:  243\n",
      "starting batch:  244\n",
      "starting batch:  245\n",
      "starting batch:  246\n",
      "starting batch:  247\n",
      "starting batch:  248\n",
      "starting batch:  249\n",
      "starting batch:  250\n",
      "starting batch:  251\n",
      "starting batch:  252\n",
      "starting batch:  253\n",
      "starting batch:  254\n",
      "starting batch:  255\n",
      "starting batch:  256\n",
      "starting batch:  257\n",
      "starting batch:  258\n",
      "starting batch:  259\n",
      "starting batch:  260\n",
      "starting batch:  261\n",
      "starting batch:  262\n",
      "starting batch:  263\n",
      "starting batch:  264\n",
      "starting batch:  265\n",
      "starting batch:  266\n",
      "starting batch:  267\n",
      "starting batch:  268\n",
      "starting batch:  269\n",
      "starting batch:  270\n",
      "starting batch:  271\n",
      "starting batch:  272\n",
      "starting batch:  273\n",
      "starting batch:  274\n",
      "starting batch:  275\n",
      "starting batch:  276\n",
      "starting batch:  277\n",
      "starting batch:  278\n",
      "starting batch:  279\n",
      "starting batch:  280\n",
      "starting batch:  281\n",
      "starting batch:  282\n",
      "starting batch:  283\n",
      "starting batch:  284\n",
      "starting batch:  285\n",
      "starting batch:  286\n",
      "starting batch:  287\n",
      "starting batch:  288\n",
      "starting batch:  289\n",
      "starting batch:  290\n",
      "starting batch:  291\n",
      "starting batch:  292\n",
      "starting batch:  293\n",
      "starting batch:  294\n",
      "starting batch:  295\n",
      "starting batch:  296\n",
      "starting batch:  297\n",
      "starting batch:  298\n",
      "starting batch:  299\n",
      "starting batch:  300\n",
      "starting batch:  301\n",
      "starting batch:  302\n",
      "starting batch:  303\n",
      "starting batch:  304\n",
      "starting batch:  305\n",
      "starting batch:  306\n",
      "starting batch:  307\n",
      "starting batch:  308\n",
      "starting batch:  309\n",
      "starting batch:  310\n",
      "starting batch:  311\n",
      "starting batch:  312\n",
      "starting batch:  313\n",
      "starting batch:  314\n",
      "starting batch:  315\n",
      "starting batch:  316\n",
      "starting batch:  317\n",
      "starting batch:  318\n",
      "starting batch:  319\n",
      "starting batch:  320\n",
      "starting batch:  321\n",
      "starting batch:  322\n",
      "starting batch:  323\n",
      "starting batch:  324\n",
      "starting batch:  325\n",
      "starting batch:  326\n",
      "starting batch:  327\n",
      "starting batch:  328\n",
      "starting batch:  329\n",
      "starting batch:  330\n",
      "starting batch:  331\n",
      "starting batch:  332\n",
      "starting batch:  333\n",
      "starting batch:  334\n",
      "starting batch:  335\n",
      "starting batch:  336\n",
      "starting batch:  337\n",
      "starting batch:  338\n",
      "starting batch:  339\n",
      "starting batch:  340\n",
      "starting batch:  341\n",
      "starting batch:  342\n",
      "starting batch:  343\n",
      "starting batch:  344\n",
      "starting batch:  345\n",
      "starting batch:  346\n",
      "starting batch:  347\n",
      "starting batch:  348\n",
      "starting batch:  349\n",
      "starting batch:  350\n",
      "starting batch:  351\n",
      "starting batch:  352\n",
      "starting batch:  353\n",
      "starting batch:  354\n",
      "starting batch:  355\n",
      "starting batch:  356\n",
      "starting batch:  357\n",
      "starting batch:  358\n",
      "starting batch:  359\n",
      "starting batch:  360\n",
      "starting batch:  361\n",
      "starting batch:  362\n",
      "starting batch:  363\n",
      "starting batch:  364\n",
      "starting batch:  365\n",
      "starting batch:  366\n",
      "starting batch:  367\n",
      "starting batch:  368\n",
      "starting batch:  369\n",
      "starting batch:  370\n",
      "starting batch:  371\n",
      "starting batch:  372\n",
      "starting batch:  373\n",
      "starting batch:  374\n",
      "starting batch:  375\n",
      "starting batch:  376\n",
      "starting batch:  377\n",
      "starting batch:  378\n",
      "starting batch:  379\n",
      "starting batch:  380\n",
      "starting batch:  381\n",
      "starting batch:  382\n",
      "starting batch:  383\n",
      "starting batch:  384\n",
      "starting batch:  385\n",
      "starting batch:  386\n",
      "starting batch:  387\n",
      "starting batch:  388\n",
      "starting batch:  389\n",
      "starting batch:  390\n",
      "starting batch:  391\n",
      "starting batch:  392\n",
      "starting batch:  393\n",
      "starting batch:  394\n",
      "starting batch:  395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch:  396\n",
      "starting batch:  397\n",
      "starting batch:  398\n",
      "starting batch:  399\n",
      "starting batch:  400\n",
      "starting batch:  401\n",
      "starting batch:  402\n",
      "starting batch:  403\n",
      "starting batch:  404\n",
      "starting batch:  405\n",
      "starting batch:  406\n",
      "starting batch:  407\n",
      "starting batch:  408\n",
      "starting batch:  409\n",
      "starting batch:  410\n",
      "starting batch:  411\n",
      "starting batch:  412\n",
      "starting batch:  413\n",
      "starting batch:  414\n",
      "starting batch:  415\n",
      "starting batch:  416\n",
      "starting batch:  417\n",
      "starting batch:  418\n",
      "starting batch:  419\n",
      "starting batch:  420\n",
      "starting batch:  421\n",
      "starting batch:  422\n",
      "starting batch:  423\n",
      "starting batch:  424\n",
      "starting batch:  425\n",
      "starting batch:  426\n",
      "starting batch:  427\n",
      "starting batch:  428\n",
      "starting batch:  429\n",
      "starting batch:  430\n",
      "starting batch:  431\n",
      "starting batch:  432\n",
      "starting batch:  433\n",
      "starting batch:  434\n",
      "starting batch:  435\n",
      "starting batch:  436\n",
      "starting batch:  437\n",
      "starting batch:  438\n",
      "starting batch:  439\n",
      "starting batch:  440\n",
      "starting batch:  441\n",
      "starting batch:  442\n",
      "starting batch:  443\n",
      "starting batch:  444\n",
      "starting batch:  445\n",
      "starting batch:  446\n",
      "starting batch:  447\n",
      "starting batch:  448\n",
      "starting batch:  449\n",
      "starting batch:  450\n",
      "starting batch:  451\n",
      "starting batch:  452\n",
      "starting batch:  453\n",
      "starting batch:  454\n",
      "starting batch:  455\n",
      "starting batch:  456\n",
      "starting batch:  457\n",
      "starting batch:  458\n",
      "starting batch:  459\n",
      "starting batch:  460\n",
      "starting batch:  461\n",
      "starting batch:  462\n",
      "starting batch:  463\n",
      "starting batch:  464\n",
      "starting batch:  465\n",
      "starting batch:  466\n",
      "starting batch:  467\n",
      "starting batch:  468\n",
      "starting batch:  469\n",
      "starting batch:  470\n",
      "starting batch:  471\n",
      "starting batch:  472\n",
      "starting batch:  473\n",
      "starting batch:  474\n",
      "Accuracy of airplane : 76 %\n",
      "Accuracy of campfire : 81 %\n",
      "Accuracy of   key : 61 %\n",
      "Accuracy of  moon : 86 %\n",
      "Accuracy of palm tree : 88 %\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(5))\n",
    "class_total = list(0. for i in range(5))\n",
    "classes = ['airplane', 'campfire', 'key', 'moon', 'palm tree']\n",
    "mini_batch_size = 64\n",
    "num_batches = int(len(X_dev)/ (4 * mini_batch_size))\n",
    "num_remaining = len(X_dev) - num_batches * mini_batch_size\n",
    "\n",
    "with torch.no_grad():\n",
    "    for t in range(num_batches):\n",
    "        rand_indices = np.random.choice(len(X_dev), mini_batch_size)\n",
    "        sketch = torch.from_numpy(X_dev[rand_indices, :, :, :])\n",
    "        labels = torch.from_numpy(Y_dev[rand_indices])\n",
    "\n",
    "#     for i in range(len(X_dev)):\n",
    "#         sketch = torch.from_numpy(X_dev[i])\n",
    "#         label = Y_dev[i]\n",
    "        print('starting batch: ', t)\n",
    "        outputs = bestModel.forward(sketch)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        for i in range(mini_batch_size):\n",
    "            predicted = predictions[i].item()\n",
    "            label = labels[i].item()\n",
    "        \n",
    "            class_total[int(label)] += 1\n",
    "            if predicted == label:\n",
    "                class_correct[predicted] += 1\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
